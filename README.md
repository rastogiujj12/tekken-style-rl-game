# Battle Agents

A selfâ€‘play, deepâ€‘reinforcementâ€‘learning (DQN)â€“powered 2D fighting simulator built with Pygame and PyTorch. In **Battle Agents**, two fighters learn to battle each other over thousands of roundsâ€”discovering strategies, combos, and even emergent tricks like doubleâ€‘jumps.

---

## ğŸš€ Features

- **Deep Qâ€‘Networks (DQN):** Each agent has its own 5â€‘layer neural network (4 inputs â†’ 256, 256, 128, 64 hidden units â†’ 5 outputs).
- **Selfâ€‘play training:** Fighters learn by competing against each other, driving an â€œarms raceâ€ of tactics.
- **Emergent behavior:** Watch them invent hitâ€‘andâ€‘run, cornerâ€‘trap pressure, timed heavy attacks, and even doubleâ€‘jumps!
- **Replay buffer & target network:** Stable training with experience replay and periodic targetâ€‘network updates.
- **Extensible architecture:** Easily add new actions (dash, block, combos), tweak rewards, or change arena settings.

---

## ğŸ“‚ Repository Structure

```
Battle-Agents/
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ background/
â”‚   â”‚   â””â”€â”€ warrior/, wizard/ sprites
â”‚   â””â”€â”€ audio/
â”‚       â”œâ”€â”€ music.mp3
â”‚       â””â”€â”€ sword.wav, magic.wav
â”œâ”€â”€ main.py          # Game loop, window setup, score & round logic
â”œâ”€â”€ fighter.py       # DQN implementation, agent physics & actions
â”œâ”€â”€ requirements.txt # Python dependencies (pygame, torch, numpy)
â””â”€â”€ README.md        # You are here!
```

---

## ğŸ›  Getting Started

1. **Clone the repo**
   ```bash
   git clone https://github.com/buzzfit/Battle-Agents.git
   cd Battle-Agents
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the simulator**
   ```bash
   python main.py
   ```

4. **Watch the learning** â€” agents train continuously. After hundreds or thousands of rounds, they evolve sophisticated tactics that you can observe in real time.

---

---

*This simulator is fully autonomous: the agents train and play against each other without manual input.*

---

## âš™ Technical Details

- **State vector:** `[dx_norm, dy_norm, self_health, opponent_health]` (4 dims)
- **Action space:** 5 discrete actions (idle/move left/move right/jump/light/heavy attack)
- **Reward structure:**
  - +1.0 for a successful hit
  - â€“0.1 for a missed attack
  - â€“5.0 on death
- **Network & training:** Adam optimizer (lr=1e-4), Î³=0.99, Îµ-decay from 1.0 â†’ 0.1, replay buffer size=10k, batch=64, target update every 1k steps.

---

## ğŸ“ˆ Results & Observations

- **Balanced play:** After 400 rounds, agents often split wins 200/200â€”a nearâ€‘perfect draw.
- **Emergent moves:** Learned combos, baitâ€‘andâ€‘punish sequences, and doubleâ€‘jumps without explicit coding.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/<name>`)
3. Commit your changes (`git commit -m "Add <feature>"`)
4. Push to your branch (`git push origin feature/<name>`)
5. Open a Pull Request

We welcome ideas: new moves, arena shapes, multiâ€‘agent tournaments, visualizations, and more!

---

## ğŸ™ Acknowledgements

This project builds upon the original manual player-vs-player tutorial by Russs123. Thanks to [Brawler Tutorial on GitHub](https://github.com/russs123/brawler_tut) and the [YouTube walkthrough video](https://www.youtube.com/watch?v=s5bd9KMSSW4) for the inspiration and foundational code.

---

ï»¿# Battle-Agents
